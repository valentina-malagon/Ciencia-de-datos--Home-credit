# -*- coding: utf-8 -*-
"""Taller 1- Ciencia de datos

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e4sGsE57TlWRF8UIMK7A8PFO80pgfQGI

# Valentina Malagon

LaS baseS de datos de Home Credit fueron creadas por una empresa de servicios financieros no bancarios que brinda préstamos a personas que normalmente no tienen acceso a servicios bancarios tradicionales. La base de datos incluye información sobre clientes potenciales y actuales, así como detalles sobre sus préstamos, pagos y otros factores relacionados con la capacidad de pago.

El objetivo de la base de datos es ayudar a la empresa a evaluar la solvencia de los clientes y determinar su capacidad para pagar préstamos futuros. Los datos incluyen información sobre la educación, el empleo, el estado civil, la edad y otros factores que pueden influir en la capacidad de pago.

La base de datos ha sido utilizada por investigadores y científicos de datos para desarrollar modelos de aprendizaje automático que puedan predecir la probabilidad de que un cliente no pueda pagar su préstamo. Estos modelos pueden ayudar a la empresa a tomar decisiones informadas sobre quiénes son los clientes más adecuados para recibir préstamos y en qué condiciones.

La base de datos de Home Credit se ha utilizado como un conjunto de datos de referencia en varias competiciones de aprendizaje automático, incluida la competencia de Kaggle Home Credit Default Risk.

# Cargar los datos
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', None)
from google.colab import drive
drive.mount('/content/drive')
application_train=pd.read_csv("drive/MyDrive/Ciencia de datos/application_train.csv")
bureau=pd.read_csv("drive/MyDrive/Ciencia de datos/bureau.csv")
credit_card_balance=pd.read_csv("drive/MyDrive/Ciencia de datos/credit_card_balance.csv")
installments_payments=pd.read_csv("drive/MyDrive/Ciencia de datos/installments_payments.csv")
previous_application=pd.read_csv("drive/MyDrive/Ciencia de datos/previous_application.csv")
sample_submission=pd.read_csv("drive/MyDrive/Ciencia de datos/sample_submission.csv")

"""# 1.1 Revision de train"""

application_train.shape

application_train.head(1)

#Ver cuantos valores  y entradas unicas tenemos en la base de datos 
application_train.drop_duplicates(inplace=True)
print(application_train.shape)
len(application_train.SK_ID_CURR.unique())

#Las variables que son dias en train las volvemos años
application_train["YEARS"]=-application_train.DAYS_BIRTH/360
application_train["YEARS_EMPLOYED"]=-application_train.DAYS_EMPLOYED/360
application_train["YEARS_REGISTRATION"]=-application_train.DAYS_REGISTRATION/360

application_train['DAYS_EMPLOYED_PERC'] = application_train['DAYS_EMPLOYED'] / application_train['DAYS_BIRTH']
application_train['INCOME_CREDIT_PERC'] = application_train['AMT_INCOME_TOTAL'] / application_train['AMT_CREDIT']
application_train['INCOME_PER_PERSON'] = application_train['AMT_INCOME_TOTAL'] / application_train['CNT_FAM_MEMBERS']
application_train['ANNUITY_INCOME_PERC'] = application_train['AMT_ANNUITY'] / application_train['AMT_INCOME_TOTAL']

application_train['TARGET'].astype(int).plot.hist();

"""En primer lugar vale la pena resaltar que la variable objetivo tiene una distribucion muy desigual pues casi el 90% de los datos tienen un resultado de 0, lo cual indica que no hubo default """

plt.style.use('fivethirtyeight')

plt.hist(application_train['YEARS'], edgecolor = 'k', bins=25)
plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');

application_train['YEARS'].corr(application_train['TARGET'])

plt.figure(figsize=(5,4))
sns.kdeplot(application_train.loc[application_train['TARGET'] ==0, 'YEARS'], label='target ==0')
sns.kdeplot(application_train.loc[application_train['TARGET'] ==1, 'YEARS'], label='target ==1')
plt.xlabel('Age(years)'); plt.ylabel('Density'); plt.title('Distributon of Ages');

"""# Insights #1
La mayor cantidad de aplicantes se encuentran entre 30 y 45 años sin embargo, y es contra intuitivo, la relacion entre la edad realmente no esta muy correlacionada con la variable objetivo. Incluso tienen una relacion negativa y muy baja. Se puede llegar a interpretar es una diferencia significativa entre entrar en default o no al rededor de los 23 a 38 años y nuevamente se observa una separacion en la grafica de las lineas roja y azul entre los 55 y los 64
"""

correlations = application_train.corr()['TARGET'].sort_values()
print('Most Positive Correlations:\n', correlations.tail(15))
print('\nMost Negative Correlations:\n', correlations.head(15))

"""# Insights #2
Realmente ninguna variable por si sola explica, o va en contra, de manera fuerte de la variable objetivo. De todo el modelo y los datos proporcionados de train la variable que mejor explica los resultados fueron EXT_SOURCE_1,2,3 que pertenecen al estudio hecho por otras instituciones sobre las personas 
"""

import scipy.stats as ss
def cramers_v(x, y):
     confusion_matrix = pd.crosstab(x,y)
     chi2 = ss.chi2_contingency(confusion_matrix)[0]
     n = confusion_matrix.sum().sum()
     phi2 = chi2/n
     r,k = confusion_matrix.shape
     phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
     rcorr = r-((r-1)**2)/(n-1)
     kcorr = k-((k-1)**2)/(n-1)
     return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))

def cramersv_dataframe_target(dataframe,nameTarget):
    df = pd.DataFrame(columns=['v_cramer'],index=dataframe.columns.tolist())
    for col in dataframe.columns.tolist():
         print('V de Cramer para {}: '.format(col) + format(round(cramers_v(dataframe[col],dataframe[nameTarget]),3)))
         df.loc[col,'v_cramer'] = round(cramers_v(dataframe[col],dataframe[nameTarget]),3)
    return df

c=cramersv_dataframe_target(application_train,"TARGET");
print('Most Positive Correlations:\n', c.tail(15))
print('Most Negative Correlations:\n', c.head(15))

"""
Realmente ninguna variable categorica por si sola explica, o va en contra, de manera fuerte de la variable objetivo."""

application_train.groupby(['OCCUPATION_TYPE'])['DAYS_EMPLOYED'].mean()

plt.figure(figsize=(5,4))
sns.kdeplot(application_train.loc[application_train['TARGET'] ==0, 'DAYS_EMPLOYED'], label='target ==0')
sns.kdeplot(application_train.loc[application_train['TARGET'] ==1, 'DAYS_EMPLOYED'], label='target ==1')
plt.xlabel('Employment(days)'); plt.ylabel('Density'); plt.title('');

"""# Insights #3
Dependiendo de la ocupacion del aplicante se tienen diferentes promedios y durabilidades en los puestos de trabajo. Y al ser empleados por mas tiempo se podria llegar a pensar que hay profesiones que tienen mas posibilidades de entrar en default; como los meseros, bartenders, los de seguridad y los conductores y los agentes inmoviliarios
"""

#Seleccionamos las variables con las que vamos a trabajar 
train =application_train[['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER','FLAG_OWN_CAR',"OWN_CAR_AGE", 'FLAG_OWN_REALTY', 
                          'CNT_CHILDREN',"CNT_FAM_MEMBERS", 'AMT_INCOME_TOTAL','AMT_CREDIT', 'AMT_ANNUITY',"NAME_INCOME_TYPE","OCCUPATION_TYPE",
                          "NAME_EDUCATION_TYPE","NAME_FAMILY_STATUS","EXT_SOURCE_1","EXT_SOURCE_2","EXT_SOURCE_3","YEARS","YEARS_EMPLOYED"
                          ,"YEARS_REGISTRATION",'DAYS_EMPLOYED_PERC',"INCOME_CREDIT_PERC",'INCOME_PER_PERSON','ANNUITY_INCOME_PERC']]
train.head(1)

#Con que tipos de datos estamos trabajando
train.dtypes.value_counts()

train.select_dtypes('object').apply(pd.Series.nunique, axis =0)

#Ver cuantos valores nulos tenemos en las variables 
train.isna().sum()

corr=train.corr()
sns.heatmap(corr)

#x = train.groupby(['CODE_GENDER',"AMT_ANNUITY"]).agg(tasa_default=('TARGET','mean')).reset_index()
#sns.barplot(data=x,x='CODE_GENDER',y='tasa_default',hue="AMT_ANNUITY")

"""# 1.2 Revision de bureau
Todos los créditos anteriores del cliente proporcionados por otras instituciones financieras que se notificaron a Credit Bureau (para clientes que tienen un préstamo en nuestra muestra).
Por cada préstamo de nuestra muestra, hay tantas filas como créditos tenía el cliente en el Buró de Crédito antes de la fecha de solicitud.
"""

bureau.head(1)

print(bureau.shape)
print(len(bureau.SK_ID_CURR.unique()))

#Sacar la suma de todos los creditos activos de una persona 
bureau["CREDIT_SUM"]=bureau.AMT_CREDIT_SUM
bureau_t1=bureau.loc[bureau.CREDIT_ACTIVE=="Active"].groupby("SK_ID_CURR").agg(AMT_CREDIT_SUM2=("CREDIT_SUM","sum")).reset_index()
bureau_t1.head(1)

#Unimos la variable nueva a train
train1 = pd.merge(train,bureau_t1,on='SK_ID_CURR',how='left')
train1.head(1)

##Sacar la suma de la cantidad de creditos activos que debe una persona 
bureau["CREDIT_SUM_DEBT"]=bureau.AMT_CREDIT_SUM_DEBT
bureau_t2=bureau.loc[bureau.CREDIT_ACTIVE=="Active"].groupby("SK_ID_CURR").agg(AMT_CREDIT_SUM_DEBT2=("CREDIT_SUM_DEBT","sum")).reset_index()
bureau_t2.head(1)

#Unimos la variable nueva a train
train2 = pd.merge(train1,bureau_t2,on='SK_ID_CURR',how='left')
train2.head(1)

"""# 1.3 Revision installments_payments
Historial de reembolso de los créditos desembolsados anteriormente en Home Credit relacionados con los préstamos de nuestra muestra.
Hay a) una fila por cada pago efectuado y b) una fila por cada pago no efectuado.
Una fila equivale a un pago de una cuota O una cuota correspondiente a un pago de un crédito anterior en Home Credit relacionado con los préstamos de nuestra muestra.
"""

#Creamos variables para el pocentaje pagado, la diferencia en los pagos, dias de mora y dias de pago anticipado
installments_payments['PERCENTAGE_PAID'] = installments_payments['AMT_PAYMENT'] / installments_payments['AMT_INSTALMENT']
installments_payments['PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']
installments_payments['DAYS_OVERDUE'] = installments_payments['DAYS_ENTRY_PAYMENT'] - installments_payments['DAYS_INSTALMENT']
installments_payments['DAYS_BEFORE_DUE'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']
#Esto es para que si el valor da positivo se queda igual, pero si es negativo lo convierte en cero
installments_payments['DAYS_OVERDUE'] = installments_payments['DAYS_OVERDUE'].apply(lambda x: x if x > 0 else 0)
installments_payments['DAYS_BEFORE_DUE'] = installments_payments['DAYS_BEFORE_DUE'].apply(lambda x: x if x > 0 else 0)

#Creamos dataframe con las nuevas variables 
install1=installments_payments.groupby("SK_ID_CURR").agg(PERCENTAGE_PAID=("PERCENTAGE_PAID","mean"),PAYMENT_DIFF=("PAYMENT_DIFF","mean"),
 DAYS_OVERDUE=("DAYS_OVERDUE","max"), DAYS_BEFORE_DUE=("DAYS_BEFORE_DUE","max")).reset_index()
install1.head(1)

#Unimos la variable nueva a train
train3 = pd.merge(train2,install1,on='SK_ID_CURR',how='left')
train3.head(1)

"""# 1.4 Revision credit_card_balance
Instantáneas mensuales de los saldos de los créditos anteriores que el solicitante tiene en Home Credit.
Esta tabla tiene una fila por cada mes de historial de cada crédito anterior en Home Credit (crédito al consumo y préstamos en efectivo) relacionado con los préstamos de nuestra muestra - es decir, la tabla tiene (#préstamos en la muestra * # de tarjetas de crédito anteriores relativas * # de meses en los que tenemos algún historial observable para la tarjeta de crédito anterior) filas.
"""

credit_card_balance.head(1)

print((credit_card_balance).shape)
print(len(credit_card_balance.SK_ID_CURR.unique()))

credit_card_balance["AMT_CREDIT_LIMIT"]=credit_card_balance.AMT_CREDIT_LIMIT_ACTUAL
credit_balance1=credit_card_balance.groupby("SK_ID_CURR").agg(AMT_CREDIT_LIMIT=("AMT_CREDIT_LIMIT","max")).reset_index()
credit_balance1.head(4)

train4 = pd.merge(train3,credit_balance1,on='SK_ID_CURR',how='left')
train4.head(1)

"""# 1.5 Revision previous_application
Todas las solicitudes anteriores de préstamos de crédito vivienda de clientes que tienen préstamos en nuestra muestra.
Hay una fila por cada solicitud anterior relacionada con préstamos de nuestra muestra de datos.
"""

previous_application.head(1)

print((previous_application).shape)
print(len(previous_application.SK_ID_CURR.unique()))

previous_application["RATE_INTEREST_PRIMARY"]=previous_application.RATE_INTEREST_PRIMARY
previous_application["RATE_INTEREST_PRIVILEGED"]=previous_application.RATE_INTEREST_PRIVILEGED
previous_application1=previous_application.groupby("SK_ID_CURR").agg(RATE_INTEREST_PRIMARY=("RATE_INTEREST_PRIMARY","sum"),
                                                                     RATE_INTEREST_PRIVILEGED=("RATE_INTEREST_PRIVILEGED","sum")).reset_index()
train5 = pd.merge(train4,previous_application1,on='SK_ID_CURR',how='left')
train5.head(1)

"""# Revision de outliers y variables categoricas """

train5.set_index('SK_ID_CURR',inplace=True)
train5.head(1)

#Para identificar outliers
fig,ax = plt.subplots(2,3,figsize=(16,9))
sns.boxplot(data=application_train,y='AMT_INCOME_TOTAL',ax=ax[0,0])
sns.boxplot(data=application_train,y='AMT_CREDIT',ax=ax[0,1])
sns.boxplot(data=application_train,y='AMT_ANNUITY',ax=ax[0,2])
sns.boxplot(data=application_train,y='YEARS',ax=ax[1,1])
sns.boxplot(data=application_train,y='YEARS_EMPLOYED',ax=ax[1,2])
sns.boxplot(data=application_train,y='YEARS_REGISTRATION',ax=ax[1,0])

lista_col_outliers=["AMT_INCOME_TOTAL","YEARS_EMPLOYED"]

#Para modificar esos valores outliers 
for columna in lista_col_outliers:
    q1 = train5[columna].quantile(0.25)
    q3 = train5[columna].quantile(0.75)
    iqr = q3-q1 
    limite_arriba = q3 + 1.5*iqr
    limite_abajo = q1 - 1.5*iqr
    mediana = train5.loc[train5[columna]<q3 + 1.5*iqr, columna].median()
    media = train5[columna].mean()
    train5.loc[train5[columna].abs() > limite_arriba,columna] = np.nan
    train5[columna].fillna(limite_arriba, inplace=True)
    train5.loc[train5[columna].abs() < limite_abajo,columna] = np.nan
    train5[columna].fillna(limite_abajo, inplace=True)

